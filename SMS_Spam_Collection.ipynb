{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMS_Spam_Collection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPZ/m9RPLuR0poxfzNwckmJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/memetics19/Sms_spam_collection-/blob/master/SMS_Spam_Collection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-pLpC9gQvSO",
        "colab_type": "text"
      },
      "source": [
        "# Importing all the libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5cTFB4zxyyT",
        "colab_type": "code",
        "outputId": "051e1d80-25b8-47cd-aa3a-f6701a783446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "import pandas as pd \n",
        "import string \n",
        "import re\n",
        "import nltk \n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkKjW0ZcePOs",
        "colab_type": "code",
        "outputId": "d0fd3082-9051-4c38-c318-39dd8e53e549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "doc = open(\"SMSSpamCollection\").read()\n",
        "doc[0:500]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"ham\\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\nham\\tOk lar... Joking wif u oni...\\nspam\\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\nham\\tU dun say so early hor... U c already then say...\\nham\\tNah I don't think he goes to usf, he lives around here though\\nspam\\tFreeMsg Hey there darling it's been 3 week's now and no word bac\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKarzIXFpxt_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parsed_data=doc.replace('\\t','\\n').split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duMABu5EqS-y",
        "colab_type": "code",
        "outputId": "a3a302f0-80b4-494a-ba27-10407653d292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "label_list=parsed_data[0::2]\n",
        "msg_list=parsed_data[1::2]\n",
        "print(label_list[0:5])\n",
        "print(msg_list[0:5])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ham', 'ham', 'spam', 'ham', 'ham']\n",
            "['Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...', 'Ok lar... Joking wif u oni...', \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\", 'U dun say so early hor... U c already then say...', \"Nah I don't think he goes to usf, he lives around here though\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4AmRp8dzLSQ",
        "colab_type": "text"
      },
      "source": [
        "# Read the Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Md1-5Eyudca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.DataFrame({\n",
        "    'label':label_list[:-1],\n",
        "    'sms':msg_list\n",
        "    \n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iivTodYuv49",
        "colab_type": "code",
        "outputId": "c297007e-9a58-400b-f2f1-e685aa8b6741",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                                sms\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5Uvii3Av2Fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=pd.read_csv(\"SMSSpamCollection\",sep=\"\\t\",header=None)\n",
        "data.columns=['label','sms']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZCaj7dc3qA3",
        "colab_type": "code",
        "outputId": "fb26c11e-eb9a-42de-8dfc-4b24716d259f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                                sms\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWjw2JlOzXdp",
        "colab_type": "text"
      },
      "source": [
        "# Shape of the Datasets \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpLpeKUhx3II",
        "colab_type": "code",
        "outputId": "08766e69-6cd0-4638-8955-d9edcfcbb616",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f\"input data as {len(data)} rows and {len(data.columns)} columns\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input data as 5572 rows and 2 columns\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jYxvNvszvTn",
        "colab_type": "text"
      },
      "source": [
        "# Length of the ham and spam "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8QD0O9ozuXA",
        "colab_type": "code",
        "outputId": "d9c25383-a027-48f9-b8b4-f8031f98b5c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f'ham= {len(data[data[\"label\"]==\"ham\"])}')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ham= 4825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4e7IrVK3yCJ",
        "colab_type": "code",
        "outputId": "c7ae0c4f-ff26-4231-f389-41ca0074447e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(f'spam= {len(data[data[\"label\"]==\"spam\"])}')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spam= 747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL_TKiKl4H8q",
        "colab_type": "text"
      },
      "source": [
        "Let's check for missing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdExRnQd33j7",
        "colab_type": "code",
        "outputId": "2c9a6ab2-f2df-49d3-d3a1-5b5ec0bfc431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(f\"number of missing label = {data['label'].isnull().sum()}\")\n",
        "print(f\"number of missing messages  = {data['sms'].isnull().sum()}\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of missing label = 0\n",
            "number of missing messages  = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcgC4DpCG_VW",
        "colab_type": "text"
      },
      "source": [
        "# Removing the Punctuation \n",
        "Punctuation is present in the string library. Let's see the punctuations present in it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08d0accMHHsZ",
        "colab_type": "code",
        "outputId": "58b9a3ab-32be-443b-b735-fb4d0745b70e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "string.punctuation"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpvgL3J9KGxs",
        "colab_type": "text"
      },
      "source": [
        "using list comprehension feature removing the punctuation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLlSbcMRJ4Rf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rem_punctate(txt):\n",
        "  txt_nopunt=\"\".join([ c for c in txt if c not in string.punctuation])\n",
        "  return txt_nopunt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwjKggbeKr67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['sms_clean']=data['sms'].apply(lambda x: rem_punctate(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjC7QXrxLgoL",
        "colab_type": "code",
        "outputId": "6f58ebcd-1f30-40f3-c9e9-7a2ef3a8c745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sms</th>\n",
              "      <th>sms_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>Go until jurong point crazy Available only in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                          sms_clean\n",
              "0   ham  ...  Go until jurong point crazy Available only in ...\n",
              "1   ham  ...                            Ok lar Joking wif u oni\n",
              "2  spam  ...  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  ...        U dun say so early hor U c already then say\n",
              "4   ham  ...  Nah I dont think he goes to usf he lives aroun...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5v7uAHBNGZP",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization \n",
        "splitting text into list of words \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR8UAlNjNE2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(txt):\n",
        "  tokens=re.split(\"\\W+\",txt)# \"W+\" is represented for non words and \"+\" represents one or more words\n",
        "  return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cjq7Gm0NOe5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[\"sms_clean_tokenized\"]=data[\"sms_clean\"].apply(lambda x:tokenize(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy2YemZfO0i-",
        "colab_type": "code",
        "outputId": "182ba1f1-3132-4dd0-f1d7-4a6103b2877c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sms</th>\n",
              "      <th>sms_clean</th>\n",
              "      <th>sms_clean_tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>Go until jurong point crazy Available only in ...</td>\n",
              "      <td>[Go, until, jurong, point, crazy, Available, o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
              "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                sms_clean_tokenized\n",
              "0   ham  ...  [Go, until, jurong, point, crazy, Available, o...\n",
              "1   ham  ...                     [Ok, lar, Joking, wif, u, oni]\n",
              "2  spam  ...  [Free, entry, in, 2, a, wkly, comp, to, win, F...\n",
              "3   ham  ...  [U, dun, say, so, early, hor, U, c, already, t...\n",
              "4   ham  ...  [Nah, I, dont, think, he, goes, to, usf, he, l...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ytxb3RWeP9dc",
        "colab_type": "text"
      },
      "source": [
        "# Removing the stop words\n",
        "\n",
        "stop words are the words are \"is\",\"the\",\"am\".. etc...\n",
        "\n",
        "It should be removed because it will make the data less which is feasible for machine learning model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4OfYX5YP8d0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words=nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMgCZ1pxR7SH",
        "colab_type": "text"
      },
      "source": [
        "let's see 10 stop words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJmft7icRk0S",
        "colab_type": "code",
        "outputId": "460c3db9-8bac-45a9-dafa-5be118f9c1f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "stop_words[0:10]"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkhadSP1SFKz",
        "colab_type": "text"
      },
      "source": [
        "let's see all the stopwords "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdOHsFomSERD",
        "colab_type": "code",
        "outputId": "0d7a1a72-51a7-4598-8bfd-c7a9d02e899b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "stop_words"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huXZ5FnmSO9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rem_stopwords(txt_tokenized):\n",
        "  txt_clean=[word for word in txt_tokenized if word not in stop_words]\n",
        "  return txt_clean\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyJ6LLlsWDyS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[\"sms_no_sw\"]=data[\"sms_clean_tokenized\"].apply(lambda x: rem_stopwords(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa7T15VzaNym",
        "colab_type": "code",
        "outputId": "e205c840-a30e-4d77-8703-69bc2d70e7ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sms</th>\n",
              "      <th>sms_clean</th>\n",
              "      <th>sms_clean_tokenized</th>\n",
              "      <th>sms_no_sw</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>Go until jurong point crazy Available only in ...</td>\n",
              "      <td>[Go, until, jurong, point, crazy, Available, o...</td>\n",
              "      <td>[Go, jurong, point, crazy, Available, bugis, n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
              "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
              "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
              "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
              "      <td>[Nah, I, dont, think, goes, usf, lives, around...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                          sms_no_sw\n",
              "0   ham  ...  [Go, jurong, point, crazy, Available, bugis, n...\n",
              "1   ham  ...                     [Ok, lar, Joking, wif, u, oni]\n",
              "2  spam  ...  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
              "3   ham  ...      [U, dun, say, early, hor, U, c, already, say]\n",
              "4   ham  ...  [Nah, I, dont, think, goes, usf, lives, around...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSWsEWaoh4Rx",
        "colab_type": "text"
      },
      "source": [
        "#Stemming\n",
        "processing into root word by using porter stemmization algorithm "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoQq4D4Watp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ps=PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFCGhx9Bns5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stemming(tokenized_text):\n",
        "  text=[ps.stem(word) for word in tokenized_text]\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jrjlYT5sOx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[\"stemmed_data\"]=data[\"sms_no_sw\"].apply(lambda x: stemming(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8Q9p6qqsmgK",
        "colab_type": "code",
        "outputId": "3181325f-0cc8-4114-c40a-915d9d7db51e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sms</th>\n",
              "      <th>sms_clean</th>\n",
              "      <th>sms_clean_tokenized</th>\n",
              "      <th>sms_no_sw</th>\n",
              "      <th>stemmed_data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>Go until jurong point crazy Available only in ...</td>\n",
              "      <td>[Go, until, jurong, point, crazy, Available, o...</td>\n",
              "      <td>[Go, jurong, point, crazy, Available, bugis, n...</td>\n",
              "      <td>[Go, jurong, point, crazi, avail, bugi, n, gre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
              "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
              "      <td>[Ok, lar, joke, wif, u, oni]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
              "      <td>[free, entri, 2, wkli, comp, win, FA, cup, fin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
              "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
              "      <td>[U, dun, say, earli, hor, U, c, alreadi, say]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
              "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
              "      <td>[Nah, I, dont, think, goes, usf, lives, around...</td>\n",
              "      <td>[nah, I, dont, think, goe, usf, live, around, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                       stemmed_data\n",
              "0   ham  ...  [Go, jurong, point, crazi, avail, bugi, n, gre...\n",
              "1   ham  ...                       [Ok, lar, joke, wif, u, oni]\n",
              "2  spam  ...  [free, entri, 2, wkli, comp, win, FA, cup, fin...\n",
              "3   ham  ...      [U, dun, say, earli, hor, U, c, alreadi, say]\n",
              "4   ham  ...  [nah, I, dont, think, goe, usf, live, around, ...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfOnnNmCk2aC",
        "colab_type": "text"
      },
      "source": [
        "# Lemmatization\n",
        " using WordNet Lemmatization algorithm \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vu8b8L9k05v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wn=nltk.WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r53Ue45sLsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lemmatization(token_txt):\n",
        "  text=[wn.lemmatize(word) for word in token_txt]\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vO71Q91puXwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['sms_tokenized']=data['sms_no_sw'].apply(lambda x:lemmatization(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8KOwyA7unzx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "db5a7088-1c8e-4bd7-dd95-9eb9f4290010"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sms</th>\n",
              "      <th>sms_clean</th>\n",
              "      <th>sms_clean_tokenized</th>\n",
              "      <th>sms_no_sw</th>\n",
              "      <th>stemmed_data</th>\n",
              "      <th>sms_tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>Go until jurong point crazy Available only in ...</td>\n",
              "      <td>[Go, until, jurong, point, crazy, Available, o...</td>\n",
              "      <td>[Go, jurong, point, crazy, Available, bugis, n...</td>\n",
              "      <td>[Go, jurong, point, crazi, avail, bugi, n, gre...</td>\n",
              "      <td>[Go, jurong, point, crazy, Available, bugis, n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>Ok lar Joking wif u oni</td>\n",
              "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
              "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
              "      <td>[Ok, lar, joke, wif, u, oni]</td>\n",
              "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
              "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
              "      <td>[free, entri, 2, wkli, comp, win, FA, cup, fin...</td>\n",
              "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>U dun say so early hor U c already then say</td>\n",
              "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
              "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
              "      <td>[U, dun, say, earli, hor, U, c, alreadi, say]</td>\n",
              "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
              "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
              "      <td>[Nah, I, dont, think, goes, usf, lives, around...</td>\n",
              "      <td>[nah, I, dont, think, goe, usf, live, around, ...</td>\n",
              "      <td>[Nah, I, dont, think, go, usf, life, around, t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  ...                                      sms_tokenized\n",
              "0   ham  ...  [Go, jurong, point, crazy, Available, bugis, n...\n",
              "1   ham  ...                     [Ok, lar, Joking, wif, u, oni]\n",
              "2  spam  ...  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
              "3   ham  ...      [U, dun, say, early, hor, U, c, already, say]\n",
              "4   ham  ...  [Nah, I, dont, think, go, usf, life, around, t...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    }
  ]
}